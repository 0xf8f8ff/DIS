{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python373jvsc74a57bd0767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90",
   "display_name": "Python 3.7.3 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import SparkSession\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.mllib.tree import RandomForest, RandomForestModel\n",
    "from pyspark.mllib.linalg import SparseVector, Vectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml.feature import StringIndexer,VectorAssembler, VectorIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.linalg import DenseVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    " sc = SparkContext(\"local[*]\", \"RFR\")\n",
    " spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = spark.read.csv(\"rf_training.csv\", inferSchema=True, header=True)\n",
    "testing = spark.read.csv(\"rf_testing.csv\", inferSchema=True, header=True)\n",
    "training = training.drop('_c0', 'location', 'date').fillna(0)\n",
    "dates = testing.select('date').toPandas()['date']\n",
    "locations = testing.select('location').toPandas()['location']\n",
    "testing = testing.drop('_c0', 'location', 'date').fillna(0)\n",
    "feature_cols = training.schema.names[:-1]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "tmp = assembler.transform(training)\n",
    "training = tmp[\"features\", \"new_cases\"]\n",
    "\n",
    "t_assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "ttmp = t_assembler.transform(testing)\n",
    "testing = ttmp[\"features\", \"new_cases\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "24.69293212890625\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "tree = RandomForestRegressor(labelCol=\"new_cases\", featuresCol=\"features\", numTrees=1000, impurity='variance')\n",
    "model = tree.fit(training)\n",
    "predictions = model.transform(testing)\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1296\n1296\n(1296, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "predictions_col = predictions.select('prediction').toPandas()['prediction'].astype(\"int\")\n",
    "output = pd.concat([dates, locations, predictions_col], 1)\n",
    "print(len(dates))\n",
    "print(len(predictions_col))\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv(\"spark_predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ]
}